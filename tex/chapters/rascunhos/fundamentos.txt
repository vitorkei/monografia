Introdução

%Nesta seção, serão apresentadas e descritas as técnicas utilizadas para a construção da inteligência artificial deste trabalho.
%O que são redes neurais convolucionais, aprendizado por reforço e quais são suas vantagens e desvantagens para que uma computador aprenda a jogar \textit{Asteroids} enxergando apenas a tela.
%Por fim, qual o papel de cada uma delas e como se unem para o aprendizado.

/-------------/

%\section{\textit{Asteroids}}
%\label{sec:asteroids}
%
%\textit{Asteroids} é um jogo de fliperama do gênero \textit{shooter} (jogo eletrônico de tiro) lançado em novembro de 1979 pela então desenvolvedora de jogos eletrônicos Atari Inc, atualmente conhecida como Atari.
%O jogo foi inspirado em \textit{Spacewar!}, \textit{Computer Space}, \textit{Space Invaders}, e \textit{Cosmos}, sendo este último um jogo não finalizado, e teve diversas versões criadas ao longo dos anos.
%As principais diferenças entre as iterações de \textit{Asteroids} incluem a presença de naves espaciais inimigas que atiram contra o jogador, formatos e tamanhos diferentes dos asteróides e direção que os asteróides se movem.
%\textit{Asteroids} é considerado um dos primeiros grandes sucessos da era de ouro dos jogos de fliperama, época em que os jogos eletrônicos começaram a se tornar comuns na cultura popular. 

%Diversas versões deste jogo foram criadas ao longo dos anos, então serão mencionadas somentes as características da iteração utilizada neste trabalho, a do Atari2600.
%O jogador controla uma nave espacial que se encontra em um campo de asteróides e precisa atirar para destrui-los enquanto evita colisões. A dificuldade aumenta conforme os asteróides se tornam mais numerosos. Os alvos podem assumir três tamanhos e três formatos diferentes. Enquanto os tamanhos são grande (inicial), médio e pequeno, os formatos são aproximadamente os mesmos em quesito de altura e largura.

%Para este trabalho, \textit{Asteroids} foi emulado utilizando o emulador Stella, do Stella Team, e a plataforma Gym-Retro, da companhia OpenAI.

%Há cinco comandos disponíveis: mover-se para frente, girar a nave no sentido horário, girar a nave no sentido anti-horário, atirar para frente, e entrar no hiper espaço. Mover-se para frente e girar no sentido horário ou anti-horário são as principais formas de movimento disponíveis ao jogador, e atirar serve para destruir os asteróides. Mover-se no hiper espaço consiste em fazer a nave desaparecer e, depois de alguns instantes, reaparecer em um local aleatório da tela. Esse é um movimento de alto risco, pois é possível reaparecer em cima de um asteróide, resultando na perda de uma vida. Por outro lado, é útil para fugir rapidamente de situações complicadas.
%A nave também possui inércia, o que dificulta a realização de manobras como curvas e esquivas.

%A nave possui aceleração e desaceleração - ou seja, inércia. Mesmo que o jogador deixe de pressionar o botão de mover-se para frente, ele continuará em movimento por um curto período de tempo antes de parar por completo. Isso gera um grau a mais de complexidade, pois faz com que manobras de esquiva e curvas sejam mais difíceis de serem devidamente executadas.

% ---------------------------------------------------------------------------- %

%\section{Gym-Retro}
%\label{sec:gymretro}

%Gym-Retro é uma plataforma para pesquisa de aprendizado por reforços e generalização em jogos desenvolvida e mantida pela empresa de pesquisas em inteligência artificial OpenAI.
%Essa ferramenta auxilia na emulação de diversos consoles de jogos eletrônicos, como Sega Genesis, Nintendo Entertainment System (NES) e Atari2600.
%Para qualquer jogo que o usuário deseje emular, é necessário que ele tenha a ROM (\textit{Read Only Memory}) do jogo.

%Gym-Retro é uma plataforma para pesquisa de aprendizado por reforços e generalização em jogos desenvolvida e mantida pela empresa de pesquisas em inteligência artificial OpenAI. O lançamento mais recente inclui jogos do Sega Genesis, Sega Master System, Nintendo Entertainment System (NES), Super Nintendo Entertainment System (SNES) e Nintendo Game Boy, como ambientes para o desenvolvimento de IA, além de suporte preliminar para Sega Game Gear, Nintendo Game Boy Color, Nintendo Game Boy Advance e NEC TurboGrafx. Em qualquer um desses consoles, a ROM (\textit{Read Only Memory}) do jogo é necessária.
%Apesar de não ter sido utilizada neste trabalho, a plataforma disponibiliza uma ferramente que permite criar \textit{save states} (salvar um estado a partir do qual é possível continuar o jogo), encontrar locais da memória, criar cenários para o agente resolver, gravar e passar arquivos de vídeo, dentre outras funcionalidades.

%O principal motivo de esta ferramenta ter sido escolhida é o suporte ao jogo \textit{Asteroids} e pela facilidade de seu uso.

%Existem oito comandos aceitos pelo emulador e pelo jogo: \textit{UP}, \textit{DOWN}, \textit{LEFT}, \textit{RIGHT}, \textit{BUTTON}, \textit{SELECT}, \textit{RESET}, e \textit{null}.
%Também é possível inserir qualquer combinação dessas entradas ao mesmo tempo, mas nem todas serão necessariamente processadas da maneira correta, então o computador escolhe e executa apenas uma ação por \textit{frame}.
%Gym-Retro baseia-se na ferramenta Gym, desenvolvida e mantida pela OpenAI, que também tem como objetivo pesquisas em aprendizado por reforço, mas não apenas para jogos.

%Esta ferramenta foi utilizada por ter suporte para desenvolvimento de aprendizado para o jogo \textit{Asteroids} e ser de fácil uso. A plataforma permite a entrada de oito ações diferentes: \textit{UP}, \textit{DOWN}, \textit{LEFT}, \textit{RIGHT}, \textit{BUTTON}, \textit{SELECT}, \textit{RESET}, \textit{null}, sendo que a ação realizada por cada botão varia de acordo com o jogo. Como descrito anteriormente, \textit{Asteroids} utiliza apenas cinco deles: \textit{UP} (mover-se para frente), \textit{DOWN} (mover-se no híper espaço), \textit{RIGHT} (girar no sentido horário), \textit{LEFT} (mover-se no sentido anti-horário), e \textit{BUTTON} (atirar). Os demais botões (\textit{SELECT} e \textit{RESET}) possuem funções relacioandas ao sistema e não ao jogo, e \textit{null} corresponde a não realizar nenhuma ação.

% ---------------------------------------------------------------------------- %

%\section{TensorFlow}
%\label{sec:tensorflow}

%TensorFlow é um arcabouço de código aberto para computações numéricas de alta performance, desenvolvido e mantido pela Google.
%Seu núcleo de computação numérica flexível permite o uso da biblioteca em diversos campos cienctíficos.
%Oferece, em particular, grande suporte a aprendizado de máquina e aprendizado profundo, ou, como é mais conhecido, \textit{deep learning}.
%Esta ferramenta foi escolhida por oferecer uma API em Python estável, ter grande suporte, comunidade ativa, e ser de código aberto.

%Apesar de não ter sido utilizado, esta biblioteca também possui uma ferramenta de visualização de dados chamada TensorBoard.

% ---------------------------------------------------------------------------- %

%\section{Inteligência artificial (IA)}
%\label{sec:ia}

%Inteligência artificial (IA) é um dos campos de estudo mais recentes de ciência e de engenharia, tendo trabalhos no assunto sendo iniciados em meados do século XX.
%Atualmente, é composta por diversas sub-áreas, podendo ser mais genérico, como aprendizado e percepção, até mais específico, como a capacidade de jogar um jogo, provar teoremas matemáticos, ou dirigir um carro em uma via movimentada.
%No livro Artificial Intelligence: A Morden Approach, de Stuart Jonathan Russel e Peter Norvig,

%RUSSEL et al. (2010) separaram oito definições de inteligência artificial em uma tabela de duas linhas por duas colunas, com duas definições por espaço.
%A linha de cima define processo de pensamento (\textit{thought process}) e raciocínio (\textit{reasoning}), enquanto a linha de baixo define comportamento (\textit{behaviour}).
%Além disso, a coluna da esquerda mede o grau de fidelidade da inteligência quando comparado com performance humana, enquanto a da direita mede a racionalidade da performance - ou seja, se toma a ação "correta"{} dado o que o sistema sabe.
%Essa tabela está representada em \ref{table:1}.

%Em linhas gerais, as definições da coluna da esquerda dizem respeito a uma inteligência artificial que se pareça com um humano, enquanto as da direita sobre uma inteligência artificial que toma ações visando estar correta e a atingir o melhor resultado possível.
%Este trabalho terá um foco maior na categoria "\textbf{Agindo racionalmente}", pois as ações tomadas pelo agente terão como objetivo o retorno da maior recompensa possível.

%\begin{table}
%\centering
%\begin{tabular}{| p{7cm} | p{7cm} |}
%  \hline
%  \textbf{Pensando como um humano} & \textbf{Pensando racionalmente} \\ \hline
%  \textit{O empolgante novo esforço de fazer computadores pensarem, serem máquinas com mentes, no sentido completo e literal da expressão} \newline (Haugeland, 1986) & \textit{O estudo das faculdades mentais através de modelos computacionais} \newline (Charniak \& McDermott, 1985) \\ & \\ {[}\textit{A automação de}{]} \textit{atividades que são associadas ao pensamento humano, como resolução de problemas, tomada de decisão, aprendizado, ...} \newline (Hellman, 1978) & \textit{O estudo das computações que tornam possíveis a percepção, razão, e ação} \newline (Winston, 1992) \\ \hline
%  \textbf{Agindo como um humano} & \textbf{Agindo racionalmente} \\ \hline
%  \textit{A arte de criar máquinas capazes de realizar funções que requerem inteligência quando feitas por pessoas} \newline (Kurzweil, 1990) & \textit{Inteligência computacional é o estudo do design de agentes inteligentes} \newline (Poole \textit{et at}, 1998) \\ & \\ \textit{O estudo de como fazer os computadores fazerem coisas que, no momento, pessoas fazem melhor} \newline (Rich and Knight, 1991) & \textit{IA... está relacionada ao comportamento inteligente em objetos} \newline (Nilsson, 1998) \\ \hline
%\end{tabular}
%\caption{Definições para inteligência artificial. Apresentação conforme a do livro de RUSSEL et al. (2010). Traduções livres feitas pelo autor.}
%\label{table:1}
%\end{table}

%Agindo como um humano: Teste de Turing. Envolve processamento de linguagem natural, representação de conhecimento, raciocínio automatizado, aprendizado de máquina e, no caso do teste de Turing total, visão computacional e robótica. Contudo, cientistas dedicaram pouco tempo em passar nesse teste, pois acreditam ser mais importante estudar os princípios da inteligência do que duplicar uma.

%Pensando como um humano: unindo computação com psicologia, a modelagem cognitiva busca expressar a teoria de como a mente funciona em um programa de computador. Duas vertentes surgiram a partir dessa abordagem: um programa tem bons resultados em uma tarefa e, portanto, é um bom modelo da performance humana, e vice-versa. Com essa separação, foi possível as duas linhas progredirem mais rapidamente, uma contribuindo com a outra.

%Pensando racionalmente: Lógica, leis do pensamento. Esta abordagem busca criar uma inteligência artificial que consiga sempre resolver um problema contanto que as premissas corretas sejam dadas. Seguindo passos lógicas, a inteligência chega a conclusões apenas se forem consequências lógicas do que já se sabe e/ou foi concluído até o momento.

%Agindo racionalmente: Um agente racional é esperado que aja de forma que atinja o melhor resultado possível ou, se houver incerteza, o melhor resultado esperado. Esta abordagem pode ser considerada um passo além das leis do pensamento, pois consegue tomar decisões quando necessário mesmo que não haja uma escolha provadamente correta.

%\subsection{Aprendizado de máquina - \textit{Machine learning}}
%\label{sec:ml}
%https://medium.com/data-science-brigade/a-diferen%C3%A7a-entre-intelig%C3%AAncia-artificial-machine-learning-e-deep-learning-930b5cc2aa42
%https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12
%https://arxiv.org/pdf/1602.01783.pdf

%Por conta do enorme volume de dados necessários para o computador aprender, aprendizado de máquina teve grandes avanços apenas nas últimas décadas, com o avanço da Internet. Com cada vez mais pessoas tendo acesso a computadores mais rápidos e eficientes, bem como o surgimento de redes sociais, a quantidade de informação digital sendo gerada, armazenada, e disponibilizada cresceu.
%As três técnicas de aprendizado de máquina mais conhecidas são \textbf{aprendizado supervisionado}, \textbf{aprendizado não supervisionado}, e \textbf{aprendizado por reforço}.
%Aprendizado supervisionado e aprendizado por reforço serão os mais discutidos neste trabalho por serem as bases para as duas principais técnicas usadas: \textit{Deep learning} e \textit{Q-learning} respectivamente.
%Aprendizado de máquina, ou \textit{machine learning} (ML) como é mais conhecido, é um campo de ciência da computação e um dos ramos de inteligência artificial que estuda a capacidade dos computadores de aprender a realizar uma tarefa sem ser explicitamente programado para isso. Após ter a tarefa definida, o computador recebe uma grande quantidade de dados, tenta reconhecer um padrão nessa entrada e, por fim, constrói um modelo para realizar predições.

%\subsection{Aprendizado supervisionado}
%\label{sec:sl}
%https://medium.com/opensanca/aprendizagem-de-maquina-supervisionada-ou-n%C3%A3o-supervisionada-7d01f78cd80a

%É mais fácil entender aprendizado supervisionado por meio de uma alegoria. O desenvolvedor da IA é um professor que fornece exercícios, cujas respostas ele possui, para o computador, seu aluno. O computador resolve os exercícios e o professor diz se as respostas dadas por seu aluno estão corretas.
%Resolver o exercício é a tarefa delegada ao computador, os exercícios são os exemplos a partir dos quais o programa deve aprender, e a resposta que o professor tem é a saída esperada. Os exercícios com as respostas que o "professor"{} possui serão chamados de exemplos rotulados, enquanto a resposta do "aluno"{} será chamada de saída da IA.

%Aprendizado supervisionado consiste em dar uma tarefa e um conjunto de exemplos com respectivas classificações esperadas para o computador. A entrada e a saída são grandes volumes de dados e respectivas categorias, rótulos. Em outras palavras, a máquina tem um "professor" que diz se a resposta dada está certa ou errada. Um exemplo simples que ajuda a ilustrar como aprendizado supervisionado funciona é fazer um computador dizer se uma imagem é de um cachorro ou não. Após mostrar diversas imagens de cachorro, dizendo quais são cachorros e quais não são, o computador deve conseguir ver uma imagem e dizer se é um ou não. 

%Esta técnica costuma cair em dois tipos de problemas: \textbf{classificação} e \textbf{regressão}.
%Em problemas de \textbf{classificação}, deseja-se que o computador classifique corretamente uma entrada dentre duas ou mais categorias pré-determinadas. Um exemplo simples deste tipo de problema é o de classificar se uma imagem é de cachorro ou não: após mostrar milhares de imagens para o computador, dizendo quais são de cachorro e quais não são, espera-se que ele classifique corretamente ao mostrar uma nova imagem.
%Em problemas de \textbf{regressão}, é esperado que os dados de entrada sigam uma função $g$ desconhecida, e o algoritmo deve encontrar uma função $h$ que se aproxime o melhor possível de $g$. Um exemplo deste tipo de problema é fornecer a metragem e número de cômodos de uma residência, e ter o seu valor como saída da IA. Com exemplos suficientes, o programa deve conseguir determinar o preço de uma residência apenas pelos números fornecidos.

%Um problema pode ser tanto de classificação quanto de regressão a depender de como for montado. Por exemplo, ao invés de o programa dizer o preço do imóvel, ele poderia dizer se o preço é superior ou inferior a um certo valor.

%Redes neurais (\textit{neural networks}), Máquinas de Vetor de Suporte (\textit{Support Vector Machine}, SVM) e Classificadores \textit{Naive Bayes} são alguns dos algoritmos de aprendizado supervisionado mais comuns. O foco deste trabalho será em redes neurais.

%\subsubsection{Aprendizado não supervisionado}
%\label{sec:usl}
%Em aprendizado não supervisionado, o aluno recebe os exercícios sem um professor para dizer se suas respostas estão certas ou erradas. Ou seja, os exemplos fornecidos ao computador não são rotulados. A principal vantagem deste tipo de aprendizado é inferência, detecção de padrões e agrupamento de elementos semelhantes.

/--------------/

Redes neurais

%No caso deste, os neurônios da primeira camada (camada de entrada) têm como saída o número que representa cada pixel da imagem, após serem convertidos para uma escala de cinza e em um valor entre 0 e 1.
%Os quadros (\textit{frames}) de entrada também são redimensionados antes de serem passados adiante na rede para acelerar o processamento.
%Os neurônios da última camada, por outro lado, têm como saída o valor utilidade esperado\footnote{Mais detalhes sobre valor esperado na seção \hyperref[sec:rl]{Aprendizado por reforço}} de cada ação possível. Esse valor é utilizado para se tomar a decisão em cada \textit{frame}. 

%No caso deste trabalho, a entrada é o \textit{frame} atual do jogo e a saída é o valor utilidade esperado de cada ação possível (mover-se, atirar e etc).
%A saída, assim como a entrada e as camadas ocultas, é composta por neurônios, mas que representam a probabilidade de cada decisão ser tomada. Para a IA deste trabalho, existem 8 neurônios na saída, um para cada ação (ainda que não seja diretamente relacionada ao jogo, como \textit{RESET} e \textit{null}) e a respectiva recompensa esperada por se tomar aquela ação naquele \textit{frame}. Portanto, é comum, principalmente no início do treinamento, haver múltiplas ações com recompensa esperada alta, ou seja, múltiplas ações que a IA considera como uma boa escolha.

%, normalmente $[0, 1]$ ou $[-1, +1]$, a depender da função,

%Apesar de as contas serem as mesmas quando feitas individualmente, esse formato é mais conveniente e eficiente de se programar por haver diversas bibliotecas que otimizam operações matriciais, além de ser mais fácil e rápido de ler e entender.

%Dependendo do problema, essa soma pode assumir qualquer valor real. Contudo, é mais desejado que seja um valor entre 0 e 1 na maioria dos casos. Por conta disso, utiliza-se funções que convertem o resultado da soma para o intervalo de interesse chamadas de funções de ativação. Alguns exemplos de funções que eram ou são comumente utilizadas para isso são a sigmoide (também conhecida como curva logistica), ReLU (\textit{Rectified Linear Unit}) e ELU (\textit{Exponential Linear Unit}).

%No início, múltiplas saídas serão consideradas como boas respostas (alta probabilidade ou recompensa esperada), pois a IA tem comportamento aleatório.
%Para que o computador saiba o quão ruim foi sua saída, é definida uma função de erro (também conhecida como função de custo), comumente calculada pelo erro quadrático médio, que consiste em subtrair o valor devolvido pela IA pelo valor esperado para aquele neurônio da camada de saída e elevar o resultado ao quadrado.
%Após essas operações serem realizadas em cada neurônio da camada de saída, os resultados são somados e obtém-se um valor numérico que mede o quão errada a IA estava para um determinado exemplo.

%Seja $M$ o número de exemplos dados para a rede neural, $S$ o número de neurônios na camada de saída, $h_{i}$ o vetor que representa a saída da IA para o $i$-ésimo exemplo, $g_{i}$ o vetor que representa a saída esperada para o $i$-ésimo exemplo. O erro quadrático médio é dado por $E_{EQM}$:

%\begin{equation} \label{eq:sme01}
%\epsilon = (h_{i} - g_{i})^{2}
%\end{equation}
%
%\begin{equation} \label{eq:sme02}
%e_{i} = \sum_{j=1}^{S}\epsilon_{j}, \qquad \epsilon_{j} = \text{ j-ésimo elemento do vetor }\epsilon
%\end{equation}
%
%\begin{equation} \label{eq:sme03}
%E_{EQM} = \frac{1}{M} \sum_{i=1}^{M}e_{i}
%\end{equation}

%Percebe-se que esse procedimento para avaliar o desempenho do código é uma forma de encapsular a rede neural em uma função que tem como entrada os pesos ($W$) e viés ($b$) de cada neurônio e, como saída, o quão bom ou ruim eles são ($E_{EQM}$).
%Por ser uma função, é possível calcular "para onde ela deve se mover"{} de forma que minimize a saída utilizando um método chamado de gradiente descendente (\textit{gradient descent}).
%Gradiente é uma generalização de derivada para multiplas variáveis que representa a inclinação da tangente da função no ponto dado e aponta para a direção de maior crescimento, direção para onde a função deve "se mover"{} para chegar no máximo local.
%Consequentemente, se tomar o valor negativo do gradiente da função, obtém-se a direção de maior decrescimento, direção do mínimo local.
%Em outras palavras, gradiente descendente é um algoritmo utilizado para encontrar o mínimo local de uma função, que é quase o que se deseja.
%O melhor caso para a função de erro seria encontrar o mínimo global, mas como isso nem sempre é possível e dificilmente alcançável, em parte por não haver garantias de que se encontrou o mínimo global, encontrar o melhor mínimo local é o suficiente.

/------------/

%\textit{Deep learning} é um dos ramos de aprendizado de máquina (\textit{machine learning}, ML) e
%\textit{Deep learning} é um ramo de aprendizado de máquina cujos algoritmos são inspirados na estrutura e funcionamento do cérebro.

%Uma das partes mais peculiares sobre \textit{deep learning} é o fato de muitos dos avanços feitos nos últimos anos serem resultado de tentativa e erro, e não de formalização e demonstração da teoria. Ainda não se entende perfeitamente o que faz uma rede neural profunda funcionar tão bem, e as características na construção de um modelo consideradas como essenciais mudam rapidamente com o passar do tempo, com algumas inclusive passando a ser consideradas estorvos ao invés de atributos chave para acelerar e aprimorar o aprendizado.

% ---------------------------------------------------------------------------- %

CNN

%Cada uma dessas duas etapas podem ter diversas camadas: na de convolução, pode haver mais de uma camada de convolução; e na de previsão, é possível haver mais de uma camada \textit{fully-conected}.

% ---------------------------------------------------------------------------- %

Processo de Decisão de Markov

%Entretanto, essas fórmulas são aplicáveis somente quando as funções de reforço $R$ e de probabilidade de transição $P$ são conhecidas, que não é o caso do jogo \textit{Asteroids}.

%Para lidar com esse problema, foi adotado o uso de \textit{\textbf{Q-learning}}.

% ---------------------------------------------------------------------------- %

Aprendizado por reforço

%Aprendizado por reforço (\textit{reinforcement learning}) é uma técnica de aprendizado de inteligência artificial e uma das bases da utilizada neste trabalho.

%Quando não é possível fazer o mapeamento das ações para cada estado do espaço de estados em que o agente se encotra, a solução adotada é fazer a IA que aprenda \textbf{características} do ambiente.

%Aprendizado por reforço, diferente do supervisionado, não recebe exemplos com rótulos que dizem qual a resposta esperada como entrada.

%Ao invés disso, a IA, normalmente chamada de agente, interage com um ambiente e recebe recompensas positivas ou negativas por suas ações.

%Seu objetivo costuma ser tomar a ação com maior recompensa esperada para cada estado - ou seja, tomar a ação que acredite ser a melhor em cada instante.

%Para domínios mais simples, como Jogo da velha, é possível determinar qual a ação com maior recompensa esperada para cada estado, a ação com maior probabilidade de vitória, por meio de uma árvore de decisão.

%Conforme o domínio se torna mais complexo, fazer esse mapeamento se torna inviável por conta da quantidade de estados que precisariam ser armazenado, como é o caso do jogo \textit{Asteroids}.

%Para se ter uma noção, um pixel que esteja diferente já é considerado um estado novo para o agente.

%Além disso, é comum haver situações em que não é possível determinar qual ação retornará a maior recompensa.

%Nesses casos, é mais viável criar e treinar um agente que aprenda a se comportar no ambiente em que está inserido do que informar se cada uma de suas ações em cada um dos estados possíveis é boa ou ruim.

%Essa abordagem é conhecida como \textbf{Processo de Decisão de Markov} (\textit{Markov Decision Process} (MDP)) para ambientes desconhecidos.


% ---------------------------------------------------------------------------- %

aprimorando o aprendizado

%Essa segunda rede também é utilizada com \textit{\textbf{Double Deep Q-Learning}}.

%***********%

%\subsection{\textit{Double Deep Q-Learning}}
%\label{sec:ddql}
%
%\textit{Double Deep Q-Learning} é uma aplicação de \textit{double Q-learning} para \textit{deep Q-learning}.
%Esta técnica utiliza duas redes neurais: uma que escolhe a melhor ação a ser tomada e uma que calcula o Q-valor dessa ação, a mesma da técnica \hyperref[sec:ft]{alvo fixo}.
%

%Como o Q-valor é uma avaliação da qualidade de uma ação, se ações sub-ótimas frequentemente receberem Q-valores maiores que a ação ótima, o agente terá dificuldades em saber qual a melhor decisão a se tomar em cada instante.
%Isso ocorre em particular no começo do treinamento, quando não se tem informação suficiente sobre as ações e existe ruído, tornando o aprendizado instável e lento.
%
%A solução proposta por \textit{double deep Q-learning} é separar a escolha da ação $A$ no estado $S$ (feita por uma rede QN) do cálculo do Q-valor alvo (feita por uma rede TN).
%Lembrando da forma como se \hyperref[eq:q_update]{calcula Q-valor}, a alteração será no fator $\max_{A'}Q^{(i)}(S',A')$, que é trocado por $Q_{TN}(S', \argmax_{A'}Q_{QN}(S',A'))$.
%Fazendo essa substituição em \ref{eq:q_update} e identificando as funções $Q()$ devidamente, chega-se à seguinte forma de se calcular o Q-valor:
%
%\begin{equation} \label{d_q_update}
%Q_{QN}^{(i+1)}(S,A) = Q_{QN}^{(i)}(S,A) + \alpha[R(S,A,S') + \gamma Q_{TN}^{(i)}(S',\argmax_{A'}Q_{QN}^{(i)}(S',A')) - Q_{QN}^{(i)}(S,A)]
%\end{equation}
%
%A atualização dos Q-valores da rede TN é feita de maneira simétrica, com $Q_{TN}$ calculando a melhor ação e $Q_{QN}$ calculando o respectivo Q-valor.

%Lembrando que o Q-valor é a recompensa esperada de se tomar uma ação $A$ em um estado $S$. Como isso depende da recompensa esperada nos estados futuros, 
%A precisão do Q-valor depende de quais ações $A$ foram tomadas e quais estados vizinhos $S'$ foram explorados para um dado estado $S$.
%No início do aprendizado, não se tem informação suficiente sobre qual a melhor ação a ser tomada, o que significa que a que tiver maior Q-valor provavelmente não será a ação ótima.

%***********%

%\subsection{\textit{Dueling Deep Q-Learning}}
%\label{sec:dueling}
