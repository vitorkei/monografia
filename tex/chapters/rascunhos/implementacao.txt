\section{\textit{Asteroids} ver. Atari2600}
\label{sec:aa2600}

A versão de \textit{Asteroids} utilizada neste trabalho é a do Atari2600, emulada pelo emulador Stella.
Nesta iteração, não existem naves espaciais inimigas, apenas asteróides que assumem três tamanhos distintos, sendo o maior deles o inicial, e três formatos diferentes, mas de aproximadamente mesma altura e largura.
Quando um asteróide grande (tamanho inicial) é destruído, outros dois de tamanho médio aparecem no lugar; após um asteróide de tamanho médio ser destruído, um de tamanho pequeno aparece em seu lugar.
Destruir um asteróide grande gera uma recompensa de 20 pontos, destruir um médio gera uma recompensa de 50, e um pequeno gera uma de 100 pontos.
A principal forma de destruir um asteróide e ganhar ponto é atirando neles, mas isso também ocorre quando há colisão entre a nave e um alvo.
Isso reduz a quantidade de vidas disponíveis e, portanto, não é um método recomendado, dado que diminui a quantidade total de pontos ganha.
Os asteróides também têm uma velocidade horizontal e vertical fixa para cada um.
A cada \textit{frame}, se movem 1 pixel na vertical e a cada aproximadamente 12 frames se movem um 1 pixel na horizontal, resultando em seus movimentos serem principalmente verticais.

O jogador possui cinco ações para jogar: mover-se para frente, girar a nave no sentido horário, girar a nave no sentido anti-horário, mover-se no hiperespaço, e atirar para frente.
Mover-se para frente e girar são as principais formas de movimento no jogo, enquanto atirar é a de destruir asteróides e ganhar pontos.
Mover-se no hiperespaço consiste em fazer a nave desaparecer por alguns instantes e reaparecer em um local aleatório da tela, podendo ser inclusive em cima de asteróides.
Portanto, é um movimento arriscado, mas útil para fugir de situações complicadas.
O jogador tem quatro vidas inicialmente.

A tela do jogo é uma matriz de tamanho 210x160 pixels com cada pixel tendo três números, que variam de 0 a 255 cada, e que determinam sua cor de acordo com a escala RGB, tendo acesso a uma paleta de 128 cores (é necessários que esses três números atinjam um certo valor para mudar o pixel de cor).
No topo da tela, há dois números indicando a pontuação total até o momento e quantidade de vidas restantes.
Desconsiderando a moldura da tela, o espaço em que o jogo ocorre é de 177x152 pixels.


No capítulo anterior, \hyperref[cap:fundamentos]{Fundamentos}, foram apresentados os dois principais conceitos para o desenvolvimento deste trabalho: \textbf{redes neurais convolucionais} e \textbf{\textit{approximate Q-learning}}.
Para a construção da inteligência artificial que jogará o jogo \textit{Asteroids}, cada uma dessas técnicas de aprendizado possui vantagens e desvantagens.
\textbf{CNNs} são capazes de aprender características dos exemplos dados, mas eles precisam estar rotulados para isso.
Uma vez que os exemplos fornecidos para o aprendizado seriam os \textit{frames} do jogo, fornecer exemplos rotulados é uma tarefa impossível, não só por haver um número incontável de estados possíveis, como não haver uma ação correta ou melhor na maioria deles.
\textbf{\textit{Approximate Q-learning}}, por outro lado, aprende sem a necessidade de exemplos rotulados, mas precisa que as características que a IA deve aprender tenham sido bem definidas pelo desenvolvedor.
Apesar de ser mais factível do que resolver o problema que a CNN enfrenta, não há garantias de que todas as características relevantes tenham sido codificadas e nem que foram bem definidas, o que limitaria a versatilidade e o aprendizado do agente.

Portanto, a solução adotada foi unir as duas técnicas: enquanto a parte de CNN se encarrega de identificar e aprender as características relevantes, a parte de aprendizado por reforço se preocupa em aprender quais as melhores ações para se tomar em cada momento.
O resultado dessa união é uma técnica chamada de \textbf{\textit{Deep Q-Learning}} ou \textbf{\textit{Deep reinforcement learning}}.
Além disso, duas técnicas adicionais são utilizadas para melhorar o aprendizado da inteligência artificial: \textit{\textbf{double} deep reinforcement learning}[1] e \textit{\textbf{dueling} deep reinforcement learning}[2], sendo o resultado chamado de \textit{\textbf{Double Dueling Deep Reinforcement Learning}} ou \textit{\textbf{Double Dueling Deep Q-Learning}}.

%--------------------%

%\section{Arquitetura}
%\label{sec:arq}

%Primeiro, foi feito um \textbf{pré-processamento} dos \textit{frames} vistos pelo agente: eles foram convertidos em escala de cinza, redimensionados para o tamanho 110x84 pixels e, por último, removidas as 12 linhas do topo e as 3 linhas do fundo da tela, por serem partes sem infomações relevantes para se aprender, como moldura, pontuação e quantidade de vidas.
%A partir de apenas um \textit{frame}, sabe-se apenas quais objetos estão na tela e onde estão, mas não é possível dizer se estão em movimento e, caso estejam, para onde vão e com que velocidade.
%Para criar uma sensação de movimento que a IA entenda, criou-se uma fila que armazena os últimos quatro \textit{frames} após serem pré-processados.

%O treinamento da inteligência artificial deste trabalho foi separado em alguns passos distintos.

%Agora que as técnicas utilizadas para o treinamento e detalhes do ambiente foram apresentados, falta descrever a arquitetura da rede e detalhes do treinamento.

%Antes de um \textit{frame} ser analisado pela IA, suas cores são convertidas para escalas de cinza.
%Em seguida, a moldura da tela do jogo é removida, ficando visível apenas a área de jogo por onde a nave e os asteroides transitam.
%Os \textit{frames} são então redimensionados para o tamanho 84x84 pixels.

%A última etapa do pré-processamento consiste em inserir os \textit{frames} em filas de tamanho quatro, com o mais antigo ficando no começo e o mais novo no final. Isso serve para que a IA tenha noção de movimento.
%Por exemplo, se ela vir uma imagem estática, não saberá para onde a nave e os asteróides estão se movendo e com que velocidade.
%Se vir quatro \textit{frames} em seguida, conseguirá inferir essas informações.
%A cada vez que a IA vê um novo \textit{frame}, o mais antigo é descartado e o novo é inserido no final.

%Após esse pré processamento, os \textit{frames} são passados para a rede neural convolucional.
%Ela possui duas camadas de convolução, com a primeira tendo 24 filtros de tamanho 12x12 e passo de tamanho 6, e a segunda tendo 48 filtros de tamanho 6x6 e passo de tamanho 3.
%Por fim, a rede possui duas camadas \textit{fully-connected}.
%Em todas as camadas, é utilizada a função de ativação ELU.
%Em seguida, calcula-se o erro quadrático médio das saídas para avaliar o desempenho da IA para o exemplo dado.
%Como explicitado anteriormente, não é possível dizer qual a melhor ação a se tomar em cada \textit{frame} do jogo e, portanto, não seria possível calcular o erro da rede.
%Para contornar esse problema, a diferença é feita entre o Q-valor desejado
%(parte $R(S,A,S') + \gamma\max_{A'}Q^{*}(S',A)$ da fórmula \ref{eq:qfunction})
%e o Q-valor previsto para a saída da rede neural.

%Entretanto, ainda há um problema a ser resolvido: qual a garantia de que a melhor ação para o próximo estado é a ação com o maior Q-valor?
%No início do treinamento, quando há poucos espaços explorados e poucas ações tomadas em cada um deles, o Q-valor é impreciso e tomar as ações com os maiores Q-valor ao invés da ação ótima dificulta o aprendizado.
%A solução adotada é utilizar duas redes neurais (portanto, \textit{\textbf{Double} Deep Q-learning}), uma que seleciona a melhor ação para o próximo estado ($\argmax_{A'}Q(S',A')$), chamada de DQN (\textit{Deep Q Network}) e outra que calcula o Q-valor utilizando a ação escolhida pela DQN ($Q(S', \argmax_{A'}Q(S',A'))$), chamada de rede alvo (\textit{target network}) neste trabalho. Dessa forma, a escolha da ação é feita separadamente do cálculo do Q-valor, tornando o treinamento mais rápido e estável.

%Seja $R(S,A,S')$ a recompensa de se tomar a ação $A$ no estado $S$ que leva para o estado $S'$ e $\gamma$ a taxa de desconto, o cálculo do Q-valor pela técnica \textit{double deep Q-learning} é dado por

%\begin{equation} \label{eq:doubledqn}
%Q(S,A) = R(S,A,S') + \gamma Q(S', \argmax_{A'}(S',A'))
%\end{equation}
%
%\textit{\textbf{Dueling} Deep Q-learning} lida com outro problema que o agente pode enfrentar:
%em uma \textit{deep Q network} normal, calcula-se o valor de cada ação em um dado estado, mas e se o estado for ruim, se, por exemplo, todas as ações levarem a morte do agente?
%Quando a ação tomada não afeta o ambiente de forma relevante e também não importa para o estado que o agente se encontra, é mais pratico não calcular o valor de cada ação nesse estado.
%Isso pode ser feito calculando separadamente os valores da recompensa de se estar em um dado estado, representada por $V(s)$, e a vantagem de se tomar uma ação $a$ no estado dado, representada por $A(s,a)$.
%Após esses dois valores serem estimados, utiliza-se uma camada para juntar os dois valores de forma que possam ser devidamente utilizados pela rede neural.
%Contudo, se a junção for feita apenas fazendo $Q(S, A;\theta, \alpha, \beta) = V(S; \theta, \beta) + A(S, A; \theta, \alpha)$, sendo $\theta$ o parâmetro das camads convolucionais e $\alpha$ e $\beta$ os parâmetros das vantagens e do valor do estado respectivamente, torna-se impossível obter qualquer um dos dois valores a partir de $Q$, o que é ruim ao fazer \textit{back propagation}.
%Essa junção é feita subtraindo a média da vantagem de todas as ações possíveis no estado em questão:

%Esta arquitetura permite que se calcule o valor de um estado sem calcular o Q-valor de cada ação desse estado, o que acelera o treinamento.
%
%O treinamento teve 30 episódios, \textit{mini-batches} de tamanho 32, probabilidade de exploração inicial 1.0 que decai até um mínimo de 0.1, taxa de aprendizado $\alpha$ igual a 0.000025, e taxa de desconto $\gamma$ igual a 0.7. O agente também recebe uma penalidade de -500 pontos a cada vida perdida.
%
%Por fim, a IA utiliza uma técnica chamada de \textit{experience replay} para melhorar o aprendizado. Ela consiste em ter uma memória que guarda experiências passadas (tuplas ($S$, $A$, $S'$ e $R$)) e utilizá-las aleatóriamente ao longo do treinamento.
%Primeiro, o agente jogará o jogo uma determinada quantidade de vezes tomando apenas ações aleatórias e armazenando as experiências na memória.
%Depois, quando estiver aprendendo, ao invés de passar o Q-valor desejado para o cálculo do erro da ação, a IA passa uma experiência passada.
%Como o ambiente é um processo de decisão Markoviano, cada ação afeta o próximo estado. Portanto, sequências de experiências estão altamente correlacionadas. Se o agente não ajustar os pesos da rede com experiências passadas, ele passará a agir apenas de acordo com o que acabou de fazer, esquecendo o que aprendeu no passado.
%A principal vantagem de utilizar \textit{experience replay} é evitar esse esquecimento.

%A tabela \ref{table:2} resume a arquitetura da IA:

%\begin{table}
%\centering
%\begin{tabular}{| p{7cm} | p{7cm} |}
%  \hline
%  \textbf{Hiper-parâmetro} & \textbf{Valor} \\ \hline
%  Dimensões dos \textit{frames} passados para a rede & 150x150 pixels \\ \hline
%  Número de \textit{frames} enfileirados & 4 \textit{frames} \\ \hline
%  Primeira camada de convolução & 24 filtros de 12x12 pixels \\ \hline
%  Segunda camada de convolução & 48 filtros de 6x6 pixels \\ \hline
%  Número de episódios & 30 episódios \\ \hline
%  Número máximo de ações por episódio & 100 000 ações \\ \hline
%  Tamanho do \textit{mini-batch} & 32 \\ \hline
%  Probabilidade de exploração inicial & 1.0 \\ \hline
%  Probabilidade mínima de exploração & 0.1 \\ \hline
%  Taxa de aprendizado $\alpha$ & 0.000025 \\ \hline
%  Taxa de desconto $\gamma$ & 0.7 \\ \hline
%  Tamanho da memória & 1 000 000 experiências \\ \hline
%  Penalidade por morte & -500 pontos \\ \hline
%\end{tabular}
%\caption{Resumo da arquitetura da inteligência artificial}
%\label{table:2}
%\end{table}
%
%
%\begin{equation} \label{eq:duelingdqn}
%Q(S,A;\theta,\alpha,\beta) = V(S;\theta,\beta) + (A(S,A;\theta,\alpha) - \frac{1}{\mathcal{A}}\sum_{A'}A(S,A;\theta,\alpha)), \text{ $\mathcal{A}$ = número de ações possíveis}
%\end{equation}
%
%
%Por fim, a inteligência artificial utiliza o modelo construído durante o treinamento para tentar jogar sozinha. Nesta etapa, não existe aleatoriedade: se a IA jogar sempre as mesmas fases nas mesmas ordens, as ações, e por consequência, a pontuação serão sempre as mesmas.
