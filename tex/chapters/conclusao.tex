% labels:
% cap:conclusoes

% ---------------------------------------------------------------------------- %
\chapter{Conclusão}
\label{cap:conclusoes}
% ---------------------------------------------------------------------------- %

Motivado pelo interesse em uma técnica de aprendizado de máquina não visto nas disciplinas de inteligência artificial da graduação, este trabalho buscou conhecer, estudar e explorar uma das formas utilizadas para ensinar um agente a se comportar em um domínio utilizando apenas imagens como entrada.
Os ambientes de características e graus de complexidade distintos permitiram avaliar as capacidades e dificuldades que essa técnica apresenta, com resultados que refletiram as expectativas ainda que apenas em parte.
Mesmo a falta de sucesso serviu como elemento de análise para este estudo.

O \textbf{\textit{Gridworld}} apresentou sucesso consistente em encontrar um caminho até o objetivo.
Isto estava dentro do esperado por conta da baixa dimensionalidade do problema:
existem poucos estados, regras e ações disponíveis, e não há aleatoriedade.
Há poucas informações para o agente aprender, podendo inclusive ser resolvido por técnicas mais simples sem grandes problemas.

O \textbf{\textit{Pong}} já se mostrou mais complicado.
Ainda que seu aprendizado tenha sido \hyperref[fig:pong_score]{promissor} para os hiper-parâmetros utilizados, ele foi lento e a arquitetura mostrou-se bem sensível, podendo não conseguir aprender por causa de pequenas alterações.
Por possuir um espaço de estados bem maior que o \textit{Gridworld}, mas bem menos situações diferentes e ações disponíveis que o \textit{Asteroids}, esse resultado refletiu o grau de complexidade médio do ambiente neste trabalho: possível, mas precisa ser feito com cuidado.
Um fato interessante é que, apesar de ainda haver espaço para o agente aprender, como é possível perceber no \hyperref[fig:pong_score]{gráfico de pontuação final do agente}, ele obteve 21 pontos em média contra a IA nativa do jogo quando o modelo foi colocado a prova, o que significa que não deixou a bola passar nenhuma vez.

Por fim, o \textbf{\textit{Asteroids}} não apresentou resultados promissores.
Sendo o ambiente mais complexo dos três, com mais regras e ações para se aprender e uma variedade maior de estados possíveis, o agente não conseguiu criar um modelo que conseguisse uma pontuação boa com as arquiteturas testadas.
Existem diversas técnicas que aceleram o aprendizado de \textit{Deep Q-Learning} e que poderiam ser aplicadas neste ambiente para tentar obter resultados positivos, como \textit{Double Deep Q-Learning}~\cite{DBLP:journals/corr/HasseltGS15}, \textit{Dueling Deep Q-Learning}~\cite{DBLP:journals/corr/WangFL15} e \textit{Rainbow}~\cite{DBLP:journals/corr/abs-1710-02298}.
Entretanto, isso seria uma garantia somente se um conjunto de hiper-parâmetros e funções boas fosse utilizado, o que já é um obstáculo por si só considerando a quantidade que existe para serem ajustados e o tempo consumido pelos treinamentos.

\textit{Deep Q-Learning} apresentou resultados positivos em comparação com o que se esperava.
Sua dificuldade de uso e tempo consumido são compensados pela capacidade de resolver problemas complexos sem que informações específicas do ambiente sejam utilizadas, como velocidade e direção de movimento de objetos da tela como da bola no \textit{Pong} ou dos asteroides no \textit{Asteroids}.
