%% ---------------------------------------------------------------------------- %
\chapter*{Resumo}
%% ---------------------------------------------------------------------------- %
%
\noindent%
TAMADA, V. K. T. \textbf{Estudo de caso de Deep Q-Learning}. Trabalho de Conclusão de Curso
 - Instituto de Matemática e Estatística, Universidade de São Paulo,
São Paulo, 2018.
\bigskip

\textit{Deep Q-Learning} é uma técnica de aprendizado de máquina que une rede neural convolucional com aprendizado por reforço para ensinar uma inteligência artificial a obter sucesso em um ambiente recebendo apenas imagens dele como entrada.
Enquanto a rede é responsável pela visualização da imagem, detecção de características e tomada de decisão, o aprendizado por reforço faz a avaliação da ação executada por meio da recompensa recebida a cada instante.
Essa forma de se aprender se assemelha a como uma pessoa aprende a realizar uma tarefa nova sem receber ajuda ou instruções, como jogar um jogo eletrônico novo.\\

O objetivo do trabalho foi estudar e avaliar a capacidade de uma inteligência artificial aprender e ter sucesso no ambiente que está inserida, assim como as dificuldades de implementar e obter bons resultados quando o \textit{Deep Q-Learning} é utilizado.
Por não ser uma técnica abordada nas disciplinas de inteligência artificial da graduação, também foi um estudo mais aprofundado de aprendizado de máquina, redes neurais e aprendizado profundo.\\

Os experimentos foram realizados em três ambientes de características e graus de complexidade distintos para verificar a flexibilidade de se ter sucesso com essa técnica e da dificuldade em obtê-lo: \textit{Gridworld}, \textit{Pong} do Atari 2600, e \textit{Asteroids} do Atari 2600, sendo o primeiro o mais simples, com poucos estados, regras, e ações possíveis, e o último o mais complexo.
O desempenho foi avaliado pelo sucesso de se alcançar o objetivo, no caso do \textit{Gridworld}, e pela pontuação obtida ao fim do jogo, no caso do \textit{Pong} e do \textit{Asteroids}.\\

Utilizando diferentes arquiteturas de rede e números de episódios de treinamento para cada ambiente, o agente obteve bons resultados no \textit{Gridworld} e no \textit{Pong}, mas não no \textit{Asteroids}.
Trabalhos relacionados indicam que \textit{Asteroids} poderia ter uma pontuação melhor se as devidas otimizações, escolhas de hiper-parâmetros e tempo suficiente de treinamento fossem utilizados.
\bigskip

\noindent
\textbf{Palavras-chave:} inteligência artificial, deep q-learning, estudo de caso

